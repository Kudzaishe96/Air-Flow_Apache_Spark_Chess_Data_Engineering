[2025-08-10T00:22:36.271+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T00:22:36.278+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T00:22:36.279+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T00:22:36.294+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T00:22:36.298+0000] {standard_task_runner.py:57} INFO - Started process 588 to run task
[2025-08-10T00:22:36.301+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmpc8grj5xc']
[2025-08-10T00:22:36.304+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask python_job
[2025-08-10T00:22:36.371+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host 95fb1ebfe85f
[2025-08-10T00:22:36.470+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T00:22:36.488+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-08-10T00:22:36.489+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master-1:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py
[2025-08-10T00:22:37.425+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T00:22:43.730+0000] {spark_submit.py:579} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-10T00:22:43.741+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO SparkContext: Running Spark version 4.0.0
[2025-08-10T00:22:43.745+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-08-10T00:22:43.747+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO SparkContext: Java version 17.0.15
[2025-08-10T00:22:43.880+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-10T00:22:43.968+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO ResourceUtils: ==============================================================
[2025-08-10T00:22:43.969+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-10T00:22:43.971+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO ResourceUtils: ==============================================================
[2025-08-10T00:22:43.973+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:43 INFO SparkContext: Submitted application: PythonWordCount
[2025-08-10T00:22:44.015+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-10T00:22:44.019+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO ResourceProfile: Limiting resource is cpu
[2025-08-10T00:22:44.021+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-10T00:22:44.084+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T00:22:44.086+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T00:22:44.087+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T00:22:44.088+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T00:22:44.091+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T00:22:44.419+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO Utils: Successfully started service 'sparkDriver' on port 37757.
[2025-08-10T00:22:44.477+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SparkEnv: Registering MapOutputTracker
[2025-08-10T00:22:44.496+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-10T00:22:44.523+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-10T00:22:44.524+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-10T00:22:44.528+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-10T00:22:44.559+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ab817b3d-4a3b-4906-aa37-eeb2244083e9
[2025-08-10T00:22:44.580+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-10T00:22:44.734+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-10T00:22:44.814+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-10T00:22:44.825+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-08-10T00:22:44.880+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T00:22:44.880+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T00:22:44.880+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T00:22:44.881+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T00:22:44.881+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T00:22:45.080+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master-1:7077...
[2025-08-10T00:22:48.895+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:48 WARN TransportClientFactory: DNS resolution failed for spark-master-1/<unresolved>:7077 took 3787 ms
[2025-08-10T00:22:48.923+0000] {spark_submit.py:579} INFO - 25/08/10 00:22:48 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master-1:7077
[2025-08-10T00:22:48.924+0000] {spark_submit.py:579} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-08-10T00:22:48.924+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
[2025-08-10T00:22:48.924+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)
[2025-08-10T00:22:48.925+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-08-10T00:22:48.925+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2025-08-10T00:22:48.926+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2025-08-10T00:22:48.926+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)
[2025-08-10T00:22:48.926+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2025-08-10T00:22:48.927+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-10T00:22:48.927+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-10T00:22:48.927+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-10T00:22:48.928+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T00:22:48.928+0000] {spark_submit.py:579} INFO - Caused by: java.io.IOException: Failed to connect to spark-master-1/<unresolved>:7077
[2025-08-10T00:22:48.928+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)
[2025-08-10T00:22:48.928+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)
[2025-08-10T00:22:48.929+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)
[2025-08-10T00:22:48.929+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)
[2025-08-10T00:22:48.929+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)
[2025-08-10T00:22:48.929+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)
[2025-08-10T00:22:48.930+0000] {spark_submit.py:579} INFO - ... 4 more
[2025-08-10T00:22:48.930+0000] {spark_submit.py:579} INFO - Caused by: java.net.UnknownHostException: spark-master-1
[2025-08-10T00:22:48.930+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
[2025-08-10T00:22:48.930+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
[2025-08-10T00:22:48.931+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
[2025-08-10T00:22:48.931+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
[2025-08-10T00:22:48.931+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getByName(InetAddress.java:1256)
[2025-08-10T00:22:48.931+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)
[2025-08-10T00:22:48.932+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)
[2025-08-10T00:22:48.932+0000] {spark_submit.py:579} INFO - at java.base/java.security.AccessController.doPrivileged(AccessController.java:569)
[2025-08-10T00:22:48.932+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)
[2025-08-10T00:22:48.932+0000] {spark_submit.py:579} INFO - at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)
[2025-08-10T00:22:48.933+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
[2025-08-10T00:22:48.933+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
[2025-08-10T00:22:48.933+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
[2025-08-10T00:22:48.933+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
[2025-08-10T00:22:48.934+0000] {spark_submit.py:579} INFO - at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
[2025-08-10T00:22:48.934+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
[2025-08-10T00:22:48.934+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
[2025-08-10T00:22:48.934+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
[2025-08-10T00:22:48.934+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
[2025-08-10T00:22:48.935+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
[2025-08-10T00:22:48.935+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
[2025-08-10T00:22:48.935+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
[2025-08-10T00:22:48.935+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
[2025-08-10T00:22:48.936+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
[2025-08-10T00:22:48.936+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
[2025-08-10T00:22:48.936+0000] {spark_submit.py:579} INFO - at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
[2025-08-10T00:22:48.937+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
[2025-08-10T00:22:48.937+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
[2025-08-10T00:22:48.937+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
[2025-08-10T00:22:48.938+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
[2025-08-10T00:22:48.938+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
[2025-08-10T00:22:48.939+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
[2025-08-10T00:22:48.939+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
[2025-08-10T00:22:48.939+0000] {spark_submit.py:579} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
[2025-08-10T00:22:48.939+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
[2025-08-10T00:22:48.939+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-08-10T00:22:48.940+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-08-10T00:22:48.940+0000] {spark_submit.py:579} INFO - ... 1 more
[2025-08-10T00:23:05.076+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:05 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master-1:7077...
[2025-08-10T00:23:08.807+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:08 WARN TransportClientFactory: DNS resolution failed for spark-master-1/<unresolved>:7077 took 3729 ms
[2025-08-10T00:23:08.809+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:08 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master-1:7077
[2025-08-10T00:23:08.810+0000] {spark_submit.py:579} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-08-10T00:23:08.810+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
[2025-08-10T00:23:08.811+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)
[2025-08-10T00:23:08.811+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-08-10T00:23:08.811+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2025-08-10T00:23:08.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2025-08-10T00:23:08.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)
[2025-08-10T00:23:08.812+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2025-08-10T00:23:08.812+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-10T00:23:08.813+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-10T00:23:08.813+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-10T00:23:08.813+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T00:23:08.813+0000] {spark_submit.py:579} INFO - Caused by: java.io.IOException: Failed to connect to spark-master-1/<unresolved>:7077
[2025-08-10T00:23:08.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)
[2025-08-10T00:23:08.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)
[2025-08-10T00:23:08.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)
[2025-08-10T00:23:08.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)
[2025-08-10T00:23:08.815+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)
[2025-08-10T00:23:08.815+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)
[2025-08-10T00:23:08.815+0000] {spark_submit.py:579} INFO - ... 4 more
[2025-08-10T00:23:08.816+0000] {spark_submit.py:579} INFO - Caused by: java.net.UnknownHostException: spark-master-1
[2025-08-10T00:23:08.816+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
[2025-08-10T00:23:08.816+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
[2025-08-10T00:23:08.816+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
[2025-08-10T00:23:08.816+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
[2025-08-10T00:23:08.817+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getByName(InetAddress.java:1256)
[2025-08-10T00:23:08.817+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)
[2025-08-10T00:23:08.817+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)
[2025-08-10T00:23:08.817+0000] {spark_submit.py:579} INFO - at java.base/java.security.AccessController.doPrivileged(AccessController.java:569)
[2025-08-10T00:23:08.818+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)
[2025-08-10T00:23:08.818+0000] {spark_submit.py:579} INFO - at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)
[2025-08-10T00:23:08.819+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
[2025-08-10T00:23:08.819+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
[2025-08-10T00:23:08.820+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
[2025-08-10T00:23:08.820+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
[2025-08-10T00:23:08.820+0000] {spark_submit.py:579} INFO - at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
[2025-08-10T00:23:08.820+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
[2025-08-10T00:23:08.821+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
[2025-08-10T00:23:08.821+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
[2025-08-10T00:23:08.821+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
[2025-08-10T00:23:08.821+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
[2025-08-10T00:23:08.822+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
[2025-08-10T00:23:08.822+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
[2025-08-10T00:23:08.822+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
[2025-08-10T00:23:08.822+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
[2025-08-10T00:23:08.823+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
[2025-08-10T00:23:08.823+0000] {spark_submit.py:579} INFO - at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
[2025-08-10T00:23:08.823+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
[2025-08-10T00:23:08.823+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
[2025-08-10T00:23:08.823+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
[2025-08-10T00:23:08.824+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
[2025-08-10T00:23:08.825+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-08-10T00:23:08.825+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-08-10T00:23:08.825+0000] {spark_submit.py:579} INFO - ... 1 more
[2025-08-10T00:23:25.077+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master-1:7077...
[2025-08-10T00:23:28.809+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:28 WARN TransportClientFactory: DNS resolution failed for spark-master-1/<unresolved>:7077 took 3731 ms
[2025-08-10T00:23:28.818+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:28 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master-1:7077
[2025-08-10T00:23:28.819+0000] {spark_submit.py:579} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-08-10T00:23:28.819+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)
[2025-08-10T00:23:28.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)
[2025-08-10T00:23:28.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2025-08-10T00:23:28.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2025-08-10T00:23:28.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2025-08-10T00:23:28.822+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:110)
[2025-08-10T00:23:28.822+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2025-08-10T00:23:28.822+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-10T00:23:28.823+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-10T00:23:28.823+0000] {spark_submit.py:579} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-10T00:23:28.823+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T00:23:28.824+0000] {spark_submit.py:579} INFO - Caused by: java.io.IOException: Failed to connect to spark-master-1/<unresolved>:7077
[2025-08-10T00:23:28.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:305)
[2025-08-10T00:23:28.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:225)
[2025-08-10T00:23:28.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:237)
[2025-08-10T00:23:28.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:207)
[2025-08-10T00:23:28.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)
[2025-08-10T00:23:28.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)
[2025-08-10T00:23:28.825+0000] {spark_submit.py:579} INFO - ... 4 more
[2025-08-10T00:23:28.826+0000] {spark_submit.py:579} INFO - Caused by: java.net.UnknownHostException: spark-master-1
[2025-08-10T00:23:28.826+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
[2025-08-10T00:23:28.826+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
[2025-08-10T00:23:28.826+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
[2025-08-10T00:23:28.826+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
[2025-08-10T00:23:28.827+0000] {spark_submit.py:579} INFO - at java.base/java.net.InetAddress.getByName(InetAddress.java:1256)
[2025-08-10T00:23:28.827+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)
[2025-08-10T00:23:28.827+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)
[2025-08-10T00:23:28.827+0000] {spark_submit.py:579} INFO - at java.base/java.security.AccessController.doPrivileged(AccessController.java:569)
[2025-08-10T00:23:28.828+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)
[2025-08-10T00:23:28.828+0000] {spark_submit.py:579} INFO - at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)
[2025-08-10T00:23:28.828+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
[2025-08-10T00:23:28.829+0000] {spark_submit.py:579} INFO - at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
[2025-08-10T00:23:28.829+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
[2025-08-10T00:23:28.829+0000] {spark_submit.py:579} INFO - at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
[2025-08-10T00:23:28.829+0000] {spark_submit.py:579} INFO - at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
[2025-08-10T00:23:28.830+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
[2025-08-10T00:23:28.830+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
[2025-08-10T00:23:28.830+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
[2025-08-10T00:23:28.830+0000] {spark_submit.py:579} INFO - at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
[2025-08-10T00:23:28.830+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
[2025-08-10T00:23:28.831+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
[2025-08-10T00:23:28.831+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
[2025-08-10T00:23:28.831+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
[2025-08-10T00:23:28.831+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
[2025-08-10T00:23:28.832+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
[2025-08-10T00:23:28.832+0000] {spark_submit.py:579} INFO - at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
[2025-08-10T00:23:28.832+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
[2025-08-10T00:23:28.833+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
[2025-08-10T00:23:28.833+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
[2025-08-10T00:23:28.833+0000] {spark_submit.py:579} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
[2025-08-10T00:23:28.833+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
[2025-08-10T00:23:28.834+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
[2025-08-10T00:23:28.834+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
[2025-08-10T00:23:28.834+0000] {spark_submit.py:579} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
[2025-08-10T00:23:28.835+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
[2025-08-10T00:23:28.835+0000] {spark_submit.py:579} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-08-10T00:23:28.835+0000] {spark_submit.py:579} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-08-10T00:23:28.836+0000] {spark_submit.py:579} INFO - ... 1 more
[2025-08-10T00:23:45.076+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.
[2025-08-10T00:23:45.078+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
[2025-08-10T00:23:45.088+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46417.
[2025-08-10T00:23:45.089+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO NettyBlockTransferService: Server created on 95fb1ebfe85f:46417
[2025-08-10T00:23:45.092+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-10T00:23:45.108+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95fb1ebfe85f, 46417, None)
[2025-08-10T00:23:45.114+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManagerMasterEndpoint: Registering block manager 95fb1ebfe85f:46417 with 434.4 MiB RAM, BlockManagerId(driver, 95fb1ebfe85f, 46417, None)
[2025-08-10T00:23:45.117+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95fb1ebfe85f, 46417, None)
[2025-08-10T00:23:45.118+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95fb1ebfe85f, 46417, None)
[2025-08-10T00:23:45.243+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-10T00:23:45.509+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at <unknown>:0.
[2025-08-10T00:23:45.522+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO SparkUI: Stopped Spark web UI at http://95fb1ebfe85f:4041
[2025-08-10T00:23:45.528+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-10T00:23:45.532+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-10T00:23:45.541+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master
[2025-08-10T00:23:45.553+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-10T00:23:45.591+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-10T00:23:45.592+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO MemoryStore: MemoryStore cleared
[2025-08-10T00:23:45.593+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManager: BlockManager stopped
[2025-08-10T00:23:45.599+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-10T00:23:45.602+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-10T00:23:45.631+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:45 INFO SparkContext: Successfully stopped SparkContext
[2025-08-10T00:23:47.327+0000] {spark_submit.py:579} INFO - Traceback (most recent call last):
[2025-08-10T00:23:47.327+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 3, in <module>
[2025-08-10T00:23:47.332+0000] {spark_submit.py:579} INFO - spark = SparkSession.builder.appName("PythonWordCount").getOrCreate()
[2025-08-10T00:23:47.332+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 559, in getOrCreate
[2025-08-10T00:23:47.333+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 641, in __init__
[2025-08-10T00:23:47.334+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1627, in __call__
[2025-08-10T00:23:47.335+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-08-10T00:23:47.337+0000] {spark_submit.py:579} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.classic.SparkSession.
[2025-08-10T00:23:47.337+0000] {spark_submit.py:579} INFO - : java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
[2025-08-10T00:23:47.338+0000] {spark_submit.py:579} INFO - This stopped SparkContext was created at:
[2025-08-10T00:23:47.338+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.339+0000] {spark_submit.py:579} INFO - org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T00:23:47.339+0000] {spark_submit.py:579} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T00:23:47.340+0000] {spark_submit.py:579} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T00:23:47.340+0000] {spark_submit.py:579} INFO - java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T00:23:47.340+0000] {spark_submit.py:579} INFO - java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T00:23:47.341+0000] {spark_submit.py:579} INFO - java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T00:23:47.341+0000] {spark_submit.py:579} INFO - py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T00:23:47.341+0000] {spark_submit.py:579} INFO - py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T00:23:47.342+0000] {spark_submit.py:579} INFO - py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T00:23:47.342+0000] {spark_submit.py:579} INFO - py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T00:23:47.342+0000] {spark_submit.py:579} INFO - py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T00:23:47.342+0000] {spark_submit.py:579} INFO - py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T00:23:47.343+0000] {spark_submit.py:579} INFO - py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T00:23:47.343+0000] {spark_submit.py:579} INFO - java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T00:23:47.343+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.344+0000] {spark_submit.py:579} INFO - And it was stopped at:
[2025-08-10T00:23:47.344+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.344+0000] {spark_submit.py:579} INFO - org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)
[2025-08-10T00:23:47.344+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.345+0000] {spark_submit.py:579} INFO - The currently active SparkContext was created at:
[2025-08-10T00:23:47.345+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.345+0000] {spark_submit.py:579} INFO - (No active SparkContext.)
[2025-08-10T00:23:47.346+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.346+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)
[2025-08-10T00:23:47.346+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:124)
[2025-08-10T00:23:47.347+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:117)
[2025-08-10T00:23:47.347+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T00:23:47.347+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T00:23:47.347+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T00:23:47.348+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T00:23:47.348+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T00:23:47.348+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T00:23:47.348+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T00:23:47.349+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T00:23:47.349+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T00:23:47.349+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T00:23:47.350+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T00:23:47.350+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T00:23:47.350+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T00:23:47.350+0000] {spark_submit.py:579} INFO - 
[2025-08-10T00:23:47.517+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:47 INFO ShutdownHookManager: Shutdown hook called
[2025-08-10T00:23:47.519+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-627e6932-4f82-43bc-8546-3d783c9a0c64
[2025-08-10T00:23:47.540+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-54158e07-6d60-4cea-837e-6d329bf27e88
[2025-08-10T00:23:47.557+0000] {spark_submit.py:579} INFO - 25/08/10 00:23:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-627e6932-4f82-43bc-8546-3d783c9a0c64/pyspark-5653f12b-9f51-4f25-9ff5-8b07feb3f543
[2025-08-10T00:23:47.813+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master-1:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T00:23:47.881+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T002236, end_date=20250810T002347
[2025-08-10T00:23:47.959+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4 for task python_job (Cannot execute: spark-submit --master spark://spark-master-1:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.; 588)
[2025-08-10T00:23:48.036+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T00:23:48.146+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T09:57:17.110+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T09:57:17.118+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T09:57:17.123+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T09:57:17.134+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T09:57:17.139+0000] {standard_task_runner.py:57} INFO - Started process 333 to run task
[2025-08-10T09:57:17.143+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmpqvayq81x']
[2025-08-10T09:57:17.146+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask python_job
[2025-08-10T09:57:17.226+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host 08d0cfb138c0
[2025-08-10T09:57:17.391+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T09:57:17.406+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-08-10T09:57:17.407+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master sparking-spark-master:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py
[2025-08-10T09:57:18.731+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T09:57:25.564+0000] {spark_submit.py:579} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-10T09:57:25.578+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO SparkContext: Running Spark version 4.0.0
[2025-08-10T09:57:25.583+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-08-10T09:57:25.583+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO SparkContext: Java version 17.0.15
[2025-08-10T09:57:25.774+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-10T09:57:25.908+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceUtils: ==============================================================
[2025-08-10T09:57:25.909+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-10T09:57:25.909+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceUtils: ==============================================================
[2025-08-10T09:57:25.910+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO SparkContext: Submitted application: PythonWordCount
[2025-08-10T09:57:25.957+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-10T09:57:25.960+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceProfile: Limiting resource is cpu
[2025-08-10T09:57:25.962+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-10T09:57:26.098+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T09:57:26.099+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T09:57:26.099+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T09:57:26.100+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T09:57:26.111+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T09:57:26.810+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO Utils: Successfully started service 'sparkDriver' on port 35459.
[2025-08-10T09:57:26.865+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SparkEnv: Registering MapOutputTracker
[2025-08-10T09:57:26.896+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-10T09:57:26.939+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-10T09:57:26.940+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-10T09:57:26.947+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-10T09:57:27.015+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7f96d801-5b88-44f0-bdc4-2875bf6f72e6
[2025-08-10T09:57:27.056+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-10T09:57:27.350+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-10T09:57:27.507+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-10T09:57:27.522+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-08-10T09:57:27.606+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T09:57:27.607+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T09:57:27.608+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T09:57:27.608+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T09:57:27.609+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T09:57:27.639+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 ERROR SparkContext: Error initializing SparkContext.
[2025-08-10T09:57:27.641+0000] {spark_submit.py:579} INFO - org.apache.spark.SparkException: Could not parse Master URL: 'sparking-spark-master:7077'
[2025-08-10T09:57:27.641+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3358)
[2025-08-10T09:57:27.642+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:593)
[2025-08-10T09:57:27.642+0000] {spark_submit.py:579} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T09:57:27.643+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T09:57:27.643+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T09:57:27.643+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T09:57:27.644+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T09:57:27.644+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T09:57:27.645+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T09:57:27.645+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T09:57:27.646+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T09:57:27.647+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T09:57:27.647+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T09:57:27.648+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T09:57:27.648+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T09:57:27.649+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T09:57:27.650+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SparkContext: SparkContext is stopping with exitCode 0 from JavaSparkContext at NativeConstructorAccessorImpl.java:0.
[2025-08-10T09:57:27.663+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SparkUI: Stopped Spark web UI at http://08d0cfb138c0:4041
[2025-08-10T09:57:27.694+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-10T09:57:27.737+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-10T09:57:27.740+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO MemoryStore: MemoryStore cleared
[2025-08-10T09:57:27.743+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO BlockManager: BlockManager stopped
[2025-08-10T09:57:27.752+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-10T09:57:27.753+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 WARN MetricsSystem: Stopping a MetricsSystem that is not running
[2025-08-10T09:57:27.758+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-10T09:57:27.780+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:27 INFO SparkContext: Successfully stopped SparkContext
[2025-08-10T09:57:27.782+0000] {spark_submit.py:579} INFO - Traceback (most recent call last):
[2025-08-10T09:57:27.783+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 3, in <module>
[2025-08-10T09:57:27.786+0000] {spark_submit.py:579} INFO - spark = SparkSession.builder.appName("PythonWordCount").getOrCreate()
[2025-08-10T09:57:27.787+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 556, in getOrCreate
[2025-08-10T09:57:27.788+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/core/context.py", line 523, in getOrCreate
[2025-08-10T09:57:27.789+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/core/context.py", line 207, in __init__
[2025-08-10T09:57:27.790+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/core/context.py", line 300, in _do_init
[2025-08-10T09:57:27.791+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/core/context.py", line 429, in _initialize_context
[2025-08-10T09:57:27.793+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1627, in __call__
[2025-08-10T09:57:27.794+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-08-10T09:57:27.796+0000] {spark_submit.py:579} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
[2025-08-10T09:57:27.797+0000] {spark_submit.py:579} INFO - : org.apache.spark.SparkException: Could not parse Master URL: 'sparking-spark-master:7077'
[2025-08-10T09:57:27.797+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3358)
[2025-08-10T09:57:27.797+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:593)
[2025-08-10T09:57:27.798+0000] {spark_submit.py:579} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T09:57:27.798+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T09:57:27.798+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T09:57:27.799+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T09:57:27.799+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T09:57:27.799+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T09:57:27.800+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T09:57:27.800+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T09:57:27.800+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T09:57:27.801+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T09:57:27.801+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T09:57:27.801+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T09:57:27.802+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T09:57:27.802+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T09:57:27.802+0000] {spark_submit.py:579} INFO - 
[2025-08-10T09:57:28.017+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:28 INFO ShutdownHookManager: Shutdown hook called
[2025-08-10T09:57:28.018+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1cdb03e-df5d-4d7a-82b8-4a6cf4a3e441
[2025-08-10T09:57:28.030+0000] {spark_submit.py:579} INFO - 25/08/10 09:57:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-4bc1a78f-d2b5-4c36-9808-622edb4e7f96
[2025-08-10T09:57:28.093+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master sparking-spark-master:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T09:57:28.097+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T095717, end_date=20250810T095728
[2025-08-10T09:57:28.115+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 5 for task python_job (Cannot execute: spark-submit --master sparking-spark-master:7077 --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.; 333)
[2025-08-10T09:57:28.154+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T09:57:28.181+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T12:54:07.140+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T12:54:07.149+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T12:54:07.150+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T12:54:07.165+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T12:54:07.172+0000] {standard_task_runner.py:57} INFO - Started process 228 to run task
[2025-08-10T12:54:07.176+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmptcf78ge0']
[2025-08-10T12:54:07.179+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask python_job
[2025-08-10T12:54:07.262+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host a8625f1109c7
[2025-08-10T12:54:07.347+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T12:54:07.360+0000] {spark_submit.py:273} INFO - Could not load connection string spark-conn, defaulting to yarn
[2025-08-10T12:54:07.362+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark jobs/python/wordcountjob.py
[2025-08-10T12:54:08.003+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T12:54:09.356+0000] {spark_submit.py:579} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2025-08-10T12:54:09.356+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:640)
[2025-08-10T12:54:09.357+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:287)
[2025-08-10T12:54:09.357+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:245)
[2025-08-10T12:54:09.357+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:104)
[2025-08-10T12:54:09.358+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1101)
[2025-08-10T12:54:09.358+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1101)
[2025-08-10T12:54:09.359+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:72)
[2025-08-10T12:54:09.359+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)
[2025-08-10T12:54:09.359+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)
[2025-08-10T12:54:09.359+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2025-08-10T12:54:09.378+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T12:54:09.383+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T125407, end_date=20250810T125409
[2025-08-10T12:54:09.394+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 5 for task python_job (Cannot execute: spark-submit --master yarn --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.; 228)
[2025-08-10T12:54:09.436+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T12:54:09.481+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T13:17:31.983+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T13:17:31.992+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T13:17:31.992+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T13:17:32.003+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T13:17:32.011+0000] {standard_task_runner.py:57} INFO - Started process 212 to run task
[2025-08-10T13:17:32.014+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmpkmufsndz']
[2025-08-10T13:17:32.017+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask python_job
[2025-08-10T13:17:32.107+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host a565ada56043
[2025-08-10T13:17:32.201+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T13:17:32.217+0000] {spark_submit.py:273} INFO - Could not load connection string spark-conn, defaulting to yarn
[2025-08-10T13:17:32.219+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py
[2025-08-10T13:17:33.025+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T13:17:34.707+0000] {spark_submit.py:579} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2025-08-10T13:17:34.708+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:640)
[2025-08-10T13:17:34.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:287)
[2025-08-10T13:17:34.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:245)
[2025-08-10T13:17:34.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:104)
[2025-08-10T13:17:34.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1101)
[2025-08-10T13:17:34.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1101)
[2025-08-10T13:17:34.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:72)
[2025-08-10T13:17:34.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)
[2025-08-10T13:17:34.711+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)
[2025-08-10T13:17:34.711+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2025-08-10T13:17:34.750+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T13:17:34.754+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T131731, end_date=20250810T131734
[2025-08-10T13:17:34.767+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4 for task python_job (Cannot execute: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.; 212)
[2025-08-10T13:17:34.799+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T13:17:34.826+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T14:39:04.392+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:39:04.399+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:39:04.400+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T14:39:04.411+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T14:39:04.417+0000] {standard_task_runner.py:57} INFO - Started process 208 to run task
[2025-08-10T14:39:04.420+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmp7aenbh06']
[2025-08-10T14:39:04.424+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask python_job
[2025-08-10T14:39:04.517+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host b165964c8bb3
[2025-08-10T14:39:04.621+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T14:39:04.643+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-08-10T14:39:04.644+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py
[2025-08-10T14:39:05.364+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T14:39:07.589+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-10T14:39:07.786+0000] {spark_submit.py:579} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-10T14:39:07.791+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:07 WARN DependencyUtils: Local jar /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar does not exist, skipping.
[2025-08-10T14:39:11.190+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkContext: Running Spark version 4.0.0
[2025-08-10T14:39:11.192+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-08-10T14:39:11.193+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkContext: Java version 17.0.15
[2025-08-10T14:39:11.225+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceUtils: ==============================================================
[2025-08-10T14:39:11.226+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-10T14:39:11.227+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceUtils: ==============================================================
[2025-08-10T14:39:11.228+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkContext: Submitted application: PythonWordCount
[2025-08-10T14:39:11.262+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-10T14:39:11.267+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceProfile: Limiting resource is cpu
[2025-08-10T14:39:11.268+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-10T14:39:11.338+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T14:39:11.340+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T14:39:11.341+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T14:39:11.343+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T14:39:11.347+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T14:39:11.656+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO Utils: Successfully started service 'sparkDriver' on port 40107.
[2025-08-10T14:39:11.706+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkEnv: Registering MapOutputTracker
[2025-08-10T14:39:11.727+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-10T14:39:11.760+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-10T14:39:11.762+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-10T14:39:11.769+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-10T14:39:11.810+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c00388d4-82e5-497e-b30d-1e25f20609e8
[2025-08-10T14:39:11.847+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-10T14:39:12.042+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-10T14:39:12.153+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-08-10T14:39:12.204+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 ERROR SparkContext: Failed to add file:/opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar to Spark environment
[2025-08-10T14:39:12.205+0000] {spark_submit.py:579} INFO - java.io.FileNotFoundException: Jar /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar not found
[2025-08-10T14:39:12.205+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)
[2025-08-10T14:39:12.206+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)
[2025-08-10T14:39:12.207+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)
[2025-08-10T14:39:12.207+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)
[2025-08-10T14:39:12.207+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
[2025-08-10T14:39:12.207+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
[2025-08-10T14:39:12.208+0000] {spark_submit.py:579} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
[2025-08-10T14:39:12.208+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:538)
[2025-08-10T14:39:12.208+0000] {spark_submit.py:579} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T14:39:12.209+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T14:39:12.209+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T14:39:12.209+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T14:39:12.210+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T14:39:12.210+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T14:39:12.211+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T14:39:12.211+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T14:39:12.212+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T14:39:12.212+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T14:39:12.214+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T14:39:12.217+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T14:39:12.219+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T14:39:12.219+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T14:39:12.234+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T14:39:12.235+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T14:39:12.236+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T14:39:12.237+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T14:39:12.237+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T14:39:12.454+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-10T14:39:12.511+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 27 ms (0 ms spent in bootstraps)
[2025-08-10T14:39:12.845+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250810143912-0000
[2025-08-10T14:39:12.853+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36493.
[2025-08-10T14:39:12.854+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO NettyBlockTransferService: Server created on b165964c8bb3:36493
[2025-08-10T14:39:12.855+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-10T14:39:12.871+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b165964c8bb3, 36493, None)
[2025-08-10T14:39:12.877+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO BlockManagerMasterEndpoint: Registering block manager b165964c8bb3:36493 with 434.4 MiB RAM, BlockManagerId(driver, b165964c8bb3, 36493, None)
[2025-08-10T14:39:12.879+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b165964c8bb3, 36493, None)
[2025-08-10T14:39:12.881+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b165964c8bb3, 36493, None)
[2025-08-10T14:39:12.960+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250810143912-0000/0 on worker-20250810143714-172.18.0.5-41047 (172.18.0.5:41047) with 2 core(s)
[2025-08-10T14:39:12.962+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250810143912-0000/0 on hostPort 172.18.0.5:41047 with 2 core(s), 1024.0 MiB RAM
[2025-08-10T14:39:13.058+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-10T14:39:14.220+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250810143912-0000/0 is now RUNNING
[2025-08-10T14:39:20.623+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:20 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:58300) with ID 0, ResourceProfileId 0
[2025-08-10T14:39:20.649+0000] {spark_submit.py:579} INFO - dict_keys(['daily', 'daily960', 'live_rapid', 'live_blitz', 'live_bullet', 'live_bughouse', 'live_blitz960', 'live_threecheck', 'live_crazyhouse', 'live_kingofthehill', 'tactics', 'rush', 'battle'])
[2025-08-10T14:39:20.670+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-10T14:39:20.674+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:20 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-08-10T14:39:20.832+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:20 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:41827 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 41827, None)
[2025-08-10T14:39:22.745+0000] {spark_submit.py:579} INFO - Traceback (most recent call last):
[2025-08-10T14:39:22.746+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 84, in <module>
[2025-08-10T14:39:22.750+0000] {spark_submit.py:579} INFO - write_data(daily,"daily")
[2025-08-10T14:39:22.752+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 80, in write_data
[2025-08-10T14:39:22.755+0000] {spark_submit.py:579} INFO - df.write.mode("overwrite").jdbc(url=jdbc_url, table=f"dbo.{table}", properties=properties)
[2025-08-10T14:39:22.756+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 2347, in jdbc
[2025-08-10T14:39:22.757+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2025-08-10T14:39:22.760+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
[2025-08-10T14:39:22.761+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-08-10T14:39:22.809+0000] {spark_submit.py:579} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o138.jdbc.
[2025-08-10T14:39:22.809+0000] {spark_submit.py:579} INFO - : java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
[2025-08-10T14:39:22.809+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T14:39:22.810+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T14:39:22.810+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T14:39:22.810+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T14:39:22.811+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T14:39:22.811+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T14:39:22.811+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T14:39:22.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T14:39:22.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T14:39:22.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T14:39:22.812+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T14:39:22.813+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T14:39:22.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T14:39:22.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T14:39:22.814+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T14:39:22.815+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T14:39:22.815+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T14:39:22.815+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T14:39:22.816+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T14:39:22.816+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T14:39:22.816+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T14:39:22.817+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T14:39:22.817+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T14:39:22.817+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T14:39:22.818+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T14:39:22.818+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T14:39:22.818+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T14:39:22.818+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T14:39:22.819+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T14:39:22.819+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T14:39:22.819+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T14:39:22.819+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T14:39:22.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T14:39:22.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T14:39:22.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T14:39:22.820+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T14:39:22.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T14:39:22.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T14:39:22.821+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T14:39:22.822+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.822+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.822+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T14:39:22.823+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T14:39:22.823+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T14:39:22.823+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T14:39:22.823+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T14:39:22.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
[2025-08-10T14:39:22.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
[2025-08-10T14:39:22.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
[2025-08-10T14:39:22.824+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
[2025-08-10T14:39:22.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
[2025-08-10T14:39:22.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
[2025-08-10T14:39:22.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
[2025-08-10T14:39:22.825+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)
[2025-08-10T14:39:22.826+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)
[2025-08-10T14:39:22.826+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-08-10T14:39:22.826+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-08-10T14:39:22.826+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-08-10T14:39:22.827+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-08-10T14:39:22.827+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-08-10T14:39:22.827+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T14:39:22.827+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-08-10T14:39:22.828+0000] {spark_submit.py:579} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-08-10T14:39:22.828+0000] {spark_submit.py:579} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-08-10T14:39:22.828+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T14:39:22.829+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T14:39:22.829+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T14:39:22.829+0000] {spark_submit.py:579} INFO - Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
[2025-08-10T14:39:22.829+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T14:39:22.830+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T14:39:22.830+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T14:39:22.830+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T14:39:22.830+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T14:39:22.831+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T14:39:22.831+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T14:39:22.832+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T14:39:22.832+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T14:39:22.832+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T14:39:22.833+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T14:39:22.833+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T14:39:22.833+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T14:39:22.834+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T14:39:22.834+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T14:39:22.834+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T14:39:22.835+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T14:39:22.835+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T14:39:22.835+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T14:39:22.836+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T14:39:22.836+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T14:39:22.836+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T14:39:22.837+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T14:39:22.837+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T14:39:22.837+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T14:39:22.837+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T14:39:22.838+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T14:39:22.838+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T14:39:22.838+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T14:39:22.838+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T14:39:22.839+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T14:39:22.839+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T14:39:22.839+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T14:39:22.840+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T14:39:22.840+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T14:39:22.840+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T14:39:22.840+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T14:39:22.841+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.841+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T14:39:22.841+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T14:39:22.841+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.842+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:39:22.842+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T14:39:22.842+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T14:39:22.842+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T14:39:22.843+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T14:39:22.843+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T14:39:22.843+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
[2025-08-10T14:39:22.843+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
[2025-08-10T14:39:22.844+0000] {spark_submit.py:579} INFO - ... 20 more
[2025-08-10T14:39:22.844+0000] {spark_submit.py:579} INFO - 
[2025-08-10T14:39:23.115+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-10T14:39:23.116+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.
[2025-08-10T14:39:23.127+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO SparkUI: Stopped Spark web UI at http://b165964c8bb3:4040
[2025-08-10T14:39:23.130+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-10T14:39:23.131+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-10T14:39:23.151+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-10T14:39:23.202+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-10T14:39:23.205+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO MemoryStore: MemoryStore cleared
[2025-08-10T14:39:23.205+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO BlockManager: BlockManager stopped
[2025-08-10T14:39:23.219+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-10T14:39:23.224+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-10T14:39:23.262+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO SparkContext: Successfully stopped SparkContext
[2025-08-10T14:39:23.263+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO ShutdownHookManager: Shutdown hook called
[2025-08-10T14:39:23.265+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO ShutdownHookManager: Deleting directory /opt/***/artifacts/spark-837e1000-ff06-405c-9467-c237150a0505
[2025-08-10T14:39:23.284+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-af917408-8983-45d0-b4b7-96fe831b3e82/pyspark-f0807871-5ede-4e35-a84b-5a13ffc0c119
[2025-08-10T14:39:23.297+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-50538b50-7718-4f84-8363-bf7c2f97fa63
[2025-08-10T14:39:23.315+0000] {spark_submit.py:579} INFO - 25/08/10 14:39:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-af917408-8983-45d0-b4b7-96fe831b3e82
[2025-08-10T14:39:23.435+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T14:39:23.453+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T143904, end_date=20250810T143923
[2025-08-10T14:39:23.490+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4 for task python_job (Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.; 208)
[2025-08-10T14:39:23.527+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T14:39:23.584+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T14:45:43.411+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:45:43.418+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:45:43.419+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T14:45:43.434+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T14:45:43.439+0000] {standard_task_runner.py:57} INFO - Started process 223 to run task
[2025-08-10T14:45:43.442+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmp2l0sqmkz']
[2025-08-10T14:45:43.445+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask python_job
[2025-08-10T14:45:43.553+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host 0941e0e18279
[2025-08-10T14:45:43.632+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T14:45:43.644+0000] {spark_submit.py:273} INFO - Could not load connection string spark-conn, defaulting to yarn
[2025-08-10T14:45:43.645+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py
[2025-08-10T14:45:44.354+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T14:45:45.423+0000] {spark_submit.py:579} INFO - Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
[2025-08-10T14:45:45.423+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:640)
[2025-08-10T14:45:45.424+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:287)
[2025-08-10T14:45:45.424+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:245)
[2025-08-10T14:45:45.424+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:104)
[2025-08-10T14:45:45.425+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1101)
[2025-08-10T14:45:45.425+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1101)
[2025-08-10T14:45:45.425+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:72)
[2025-08-10T14:45:45.426+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)
[2025-08-10T14:45:45.426+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)
[2025-08-10T14:45:45.426+0000] {spark_submit.py:579} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2025-08-10T14:45:45.444+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T14:45:45.450+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T144543, end_date=20250810T144545
[2025-08-10T14:45:45.461+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4 for task python_job (Cannot execute: spark-submit --master yarn --jars /opt/spark/external-jars/spark-mssql-connector_2.11-1.1.0.jar --name arrow-spark jobs/python/wordcountjob.py. Error code is: 1.; 223)
[2025-08-10T14:45:45.500+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T14:45:45.523+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T14:57:25.075+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:57:25.082+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T14:57:25.083+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T14:57:25.098+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T14:57:25.107+0000] {standard_task_runner.py:57} INFO - Started process 202 to run task
[2025-08-10T14:57:25.112+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmp3ecbp0p3']
[2025-08-10T14:57:25.117+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask python_job
[2025-08-10T14:57:25.222+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host bbbd335a28b1
[2025-08-10T14:57:25.333+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T14:57:25.351+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-08-10T14:57:25.352+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py
[2025-08-10T14:57:26.617+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T14:57:29.233+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-10T14:57:29.404+0000] {spark_submit.py:579} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-10T14:57:29.410+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:29 WARN DependencyUtils: Local jar /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar does not exist, skipping.
[2025-08-10T14:57:32.413+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkContext: Running Spark version 4.0.0
[2025-08-10T14:57:32.416+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-08-10T14:57:32.417+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkContext: Java version 17.0.15
[2025-08-10T14:57:32.443+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceUtils: ==============================================================
[2025-08-10T14:57:32.444+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-10T14:57:32.445+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceUtils: ==============================================================
[2025-08-10T14:57:32.445+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkContext: Submitted application: PythonWordCount
[2025-08-10T14:57:32.476+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-10T14:57:32.479+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceProfile: Limiting resource is cpu
[2025-08-10T14:57:32.482+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-10T14:57:32.541+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T14:57:32.542+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T14:57:32.543+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T14:57:32.543+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T14:57:32.547+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T14:57:32.821+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO Utils: Successfully started service 'sparkDriver' on port 32855.
[2025-08-10T14:57:32.854+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkEnv: Registering MapOutputTracker
[2025-08-10T14:57:32.869+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-10T14:57:32.892+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-10T14:57:32.893+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-10T14:57:32.897+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-10T14:57:32.930+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4c10fa54-4c6d-4e5f-8126-57b9735d0f76
[2025-08-10T14:57:32.958+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-10T14:57:33.160+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-10T14:57:33.252+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-10T14:57:33.262+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-08-10T14:57:33.310+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 ERROR SparkContext: Failed to add file:/opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar to Spark environment
[2025-08-10T14:57:33.310+0000] {spark_submit.py:579} INFO - java.io.FileNotFoundException: Jar /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar not found
[2025-08-10T14:57:33.311+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)
[2025-08-10T14:57:33.311+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)
[2025-08-10T14:57:33.312+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)
[2025-08-10T14:57:33.312+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)
[2025-08-10T14:57:33.312+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
[2025-08-10T14:57:33.313+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
[2025-08-10T14:57:33.313+0000] {spark_submit.py:579} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
[2025-08-10T14:57:33.314+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:538)
[2025-08-10T14:57:33.315+0000] {spark_submit.py:579} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T14:57:33.315+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T14:57:33.316+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T14:57:33.316+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T14:57:33.317+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T14:57:33.317+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T14:57:33.318+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T14:57:33.318+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T14:57:33.318+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T14:57:33.319+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T14:57:33.320+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T14:57:33.320+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T14:57:33.320+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T14:57:33.321+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T14:57:33.351+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T14:57:33.352+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T14:57:33.352+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T14:57:33.353+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T14:57:33.353+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T14:57:33.669+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-10T14:57:33.715+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 25 ms (0 ms spent in bootstraps)
[2025-08-10T14:57:34.356+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250810145734-0000
[2025-08-10T14:57:34.368+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45771.
[2025-08-10T14:57:34.368+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO NettyBlockTransferService: Server created on bbbd335a28b1:45771
[2025-08-10T14:57:34.371+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-10T14:57:34.391+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bbbd335a28b1, 45771, None)
[2025-08-10T14:57:34.400+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO BlockManagerMasterEndpoint: Registering block manager bbbd335a28b1:45771 with 434.4 MiB RAM, BlockManagerId(driver, bbbd335a28b1, 45771, None)
[2025-08-10T14:57:34.403+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bbbd335a28b1, 45771, None)
[2025-08-10T14:57:34.404+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bbbd335a28b1, 45771, None)
[2025-08-10T14:57:34.458+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250810145734-0000/0 on worker-20250810145601-172.18.0.5-37841 (172.18.0.5:37841) with 2 core(s)
[2025-08-10T14:57:34.460+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250810145734-0000/0 on hostPort 172.18.0.5:37841 with 2 core(s), 1024.0 MiB RAM
[2025-08-10T14:57:34.608+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-10T14:57:35.972+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250810145734-0000/0 is now RUNNING
[2025-08-10T14:57:39.685+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:39 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:57794) with ID 0, ResourceProfileId 0
[2025-08-10T14:57:39.832+0000] {spark_submit.py:579} INFO - dict_keys(['daily', 'daily960', 'live_rapid', 'live_blitz', 'live_bullet', 'live_bughouse', 'live_blitz960', 'live_threecheck', 'live_crazyhouse', 'live_kingofthehill', 'tactics', 'rush', 'battle'])
[2025-08-10T14:57:39.835+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:38781 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 38781, None)
[2025-08-10T14:57:39.851+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-10T14:57:39.855+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:39 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-08-10T14:57:41.633+0000] {spark_submit.py:579} INFO - Traceback (most recent call last):
[2025-08-10T14:57:41.635+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 84, in <module>
[2025-08-10T14:57:41.638+0000] {spark_submit.py:579} INFO - write_data(daily,"daily")
[2025-08-10T14:57:41.638+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 80, in write_data
[2025-08-10T14:57:41.639+0000] {spark_submit.py:579} INFO - df.write.mode("overwrite").jdbc(url=jdbc_url, table=f"dbo.{table}", properties=properties)
[2025-08-10T14:57:41.640+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 2347, in jdbc
[2025-08-10T14:57:41.641+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2025-08-10T14:57:41.642+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
[2025-08-10T14:57:41.643+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-08-10T14:57:41.688+0000] {spark_submit.py:579} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o138.jdbc.
[2025-08-10T14:57:41.689+0000] {spark_submit.py:579} INFO - : java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
[2025-08-10T14:57:41.689+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T14:57:41.690+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T14:57:41.690+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T14:57:41.690+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T14:57:41.691+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T14:57:41.691+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T14:57:41.691+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T14:57:41.692+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T14:57:41.692+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T14:57:41.692+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T14:57:41.692+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T14:57:41.693+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T14:57:41.693+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T14:57:41.693+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T14:57:41.694+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T14:57:41.694+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T14:57:41.694+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T14:57:41.695+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T14:57:41.695+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T14:57:41.695+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T14:57:41.696+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T14:57:41.696+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T14:57:41.696+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T14:57:41.696+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T14:57:41.697+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T14:57:41.697+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T14:57:41.697+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T14:57:41.698+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T14:57:41.698+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T14:57:41.698+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T14:57:41.699+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T14:57:41.699+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T14:57:41.700+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T14:57:41.700+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T14:57:41.700+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T14:57:41.701+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T14:57:41.701+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T14:57:41.701+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.701+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T14:57:41.702+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T14:57:41.702+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.702+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.703+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T14:57:41.703+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T14:57:41.703+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T14:57:41.703+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T14:57:41.704+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T14:57:41.704+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
[2025-08-10T14:57:41.704+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
[2025-08-10T14:57:41.704+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
[2025-08-10T14:57:41.704+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
[2025-08-10T14:57:41.705+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
[2025-08-10T14:57:41.705+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
[2025-08-10T14:57:41.705+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
[2025-08-10T14:57:41.705+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)
[2025-08-10T14:57:41.705+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)
[2025-08-10T14:57:41.706+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-08-10T14:57:41.706+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-08-10T14:57:41.706+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-08-10T14:57:41.706+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-08-10T14:57:41.706+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-08-10T14:57:41.707+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T14:57:41.707+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-08-10T14:57:41.707+0000] {spark_submit.py:579} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-08-10T14:57:41.707+0000] {spark_submit.py:579} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-08-10T14:57:41.707+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T14:57:41.708+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T14:57:41.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T14:57:41.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T14:57:41.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T14:57:41.709+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T14:57:41.709+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T14:57:41.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T14:57:41.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T14:57:41.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T14:57:41.710+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T14:57:41.711+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T14:57:41.712+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T14:57:41.712+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T14:57:41.712+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T14:57:41.712+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T14:57:41.712+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T14:57:41.713+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T14:57:41.713+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T14:57:41.713+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T14:57:41.713+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T14:57:41.713+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T14:57:41.714+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T14:57:41.714+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T14:57:41.714+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T14:57:41.715+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T14:57:41.715+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T14:57:41.715+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T14:57:41.715+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T14:57:41.716+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T14:57:41.716+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T14:57:41.716+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T14:57:41.716+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T14:57:41.716+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T14:57:41.717+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T14:57:41.717+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T14:57:41.717+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.717+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T14:57:41.717+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T14:57:41.718+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.718+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T14:57:41.718+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T14:57:41.718+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T14:57:41.718+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T14:57:41.719+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T14:57:41.719+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T14:57:41.719+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
[2025-08-10T14:57:41.719+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
[2025-08-10T14:57:41.719+0000] {spark_submit.py:579} INFO - ... 20 more
[2025-08-10T14:57:41.720+0000] {spark_submit.py:579} INFO - 
[2025-08-10T14:57:41.869+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-10T14:57:41.870+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.
[2025-08-10T14:57:41.879+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO SparkUI: Stopped Spark web UI at http://bbbd335a28b1:4041
[2025-08-10T14:57:41.882+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-10T14:57:41.883+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-10T14:57:41.897+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-10T14:57:41.949+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-10T14:57:41.950+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO MemoryStore: MemoryStore cleared
[2025-08-10T14:57:41.951+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO BlockManager: BlockManager stopped
[2025-08-10T14:57:41.959+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-10T14:57:41.962+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-10T14:57:41.992+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO SparkContext: Successfully stopped SparkContext
[2025-08-10T14:57:41.993+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO ShutdownHookManager: Shutdown hook called
[2025-08-10T14:57:41.995+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:41 INFO ShutdownHookManager: Deleting directory /opt/***/artifacts/spark-d179ee54-db8c-44b4-8581-acd42a6e5446
[2025-08-10T14:57:42.010+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb5117fb-d768-40ad-b7e7-5fa9c5a70c33
[2025-08-10T14:57:42.024+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2b4ddea-1eb7-4a28-b16e-768f751cc428
[2025-08-10T14:57:42.038+0000] {spark_submit.py:579} INFO - 25/08/10 14:57:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb5117fb-d768-40ad-b7e7-5fa9c5a70c33/pyspark-9ebe1ed0-2013-429a-93ec-50abc5f2635e
[2025-08-10T14:57:42.099+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T14:57:42.110+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T145725, end_date=20250810T145742
[2025-08-10T14:57:42.132+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 4 for task python_job (Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.; 202)
[2025-08-10T14:57:42.152+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T14:57:42.199+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-10T15:03:37.324+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T15:03:37.334+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [queued]>
[2025-08-10T15:03:37.334+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-08-10T15:03:37.351+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-08-09 00:00:00+00:00
[2025-08-10T15:03:37.358+0000] {standard_task_runner.py:57} INFO - Started process 213 to run task
[2025-08-10T15:03:37.362+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'sparking_flow', 'python_job', 'scheduled__2025-08-09T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/spark_***.py', '--cfg-path', '/tmp/tmpzbfet1ex']
[2025-08-10T15:03:37.366+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask python_job
[2025-08-10T15:03:37.464+0000] {task_command.py:415} INFO - Running <TaskInstance: sparking_flow.python_job scheduled__2025-08-09T00:00:00+00:00 [running]> on host dab662e4aa8d
[2025-08-10T15:03:37.549+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Yusuf Ganiyu' AIRFLOW_CTX_DAG_ID='sparking_flow' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-08-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-08-09T00:00:00+00:00'
[2025-08-10T15:03:37.562+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-08-10T15:03:37.563+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py
[2025-08-10T15:03:38.333+0000] {spark_submit.py:579} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-10T15:03:40.204+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-10T15:03:40.357+0000] {spark_submit.py:579} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-10T15:03:40.362+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:40 WARN DependencyUtils: Local jar /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar does not exist, skipping.
[2025-08-10T15:03:43.173+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkContext: Running Spark version 4.0.0
[2025-08-10T15:03:43.176+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-08-10T15:03:43.177+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkContext: Java version 17.0.15
[2025-08-10T15:03:43.213+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceUtils: ==============================================================
[2025-08-10T15:03:43.213+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-10T15:03:43.214+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceUtils: ==============================================================
[2025-08-10T15:03:43.215+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkContext: Submitted application: PythonWordCount
[2025-08-10T15:03:43.240+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-10T15:03:43.243+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceProfile: Limiting resource is cpu
[2025-08-10T15:03:43.245+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-10T15:03:43.303+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T15:03:43.305+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T15:03:43.306+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T15:03:43.306+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T15:03:43.310+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T15:03:43.601+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO Utils: Successfully started service 'sparkDriver' on port 33801.
[2025-08-10T15:03:43.645+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkEnv: Registering MapOutputTracker
[2025-08-10T15:03:43.657+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-10T15:03:43.676+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-10T15:03:43.677+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-10T15:03:43.682+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-10T15:03:43.716+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ec233e72-8cc3-4557-ae9d-ce0a3295b0b3
[2025-08-10T15:03:43.748+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-10T15:03:43.923+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-10T15:03:44.022+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-10T15:03:44.036+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-08-10T15:03:44.082+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 ERROR SparkContext: Failed to add file:/opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar to Spark environment
[2025-08-10T15:03:44.082+0000] {spark_submit.py:579} INFO - java.io.FileNotFoundException: Jar /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar not found
[2025-08-10T15:03:44.084+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)
[2025-08-10T15:03:44.084+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)
[2025-08-10T15:03:44.085+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)
[2025-08-10T15:03:44.085+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)
[2025-08-10T15:03:44.086+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)
[2025-08-10T15:03:44.086+0000] {spark_submit.py:579} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)
[2025-08-10T15:03:44.086+0000] {spark_submit.py:579} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:935)
[2025-08-10T15:03:44.086+0000] {spark_submit.py:579} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:538)
[2025-08-10T15:03:44.087+0000] {spark_submit.py:579} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
[2025-08-10T15:03:44.087+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-08-10T15:03:44.087+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-08-10T15:03:44.088+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-08-10T15:03:44.088+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-08-10T15:03:44.088+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-08-10T15:03:44.089+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-08-10T15:03:44.090+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T15:03:44.090+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2025-08-10T15:03:44.090+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-08-10T15:03:44.091+0000] {spark_submit.py:579} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-08-10T15:03:44.091+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T15:03:44.091+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T15:03:44.092+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T15:03:44.110+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO SecurityManager: Changing view acls to: ***
[2025-08-10T15:03:44.110+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO SecurityManager: Changing modify acls to: ***
[2025-08-10T15:03:44.111+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-10T15:03:44.112+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-10T15:03:44.113+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-10T15:03:44.275+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-10T15:03:44.329+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 26 ms (0 ms spent in bootstraps)
[2025-08-10T15:03:44.917+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250810150344-0000
[2025-08-10T15:03:44.925+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39737.
[2025-08-10T15:03:44.926+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO NettyBlockTransferService: Server created on dab662e4aa8d:39737
[2025-08-10T15:03:44.928+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-10T15:03:44.952+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dab662e4aa8d, 39737, None)
[2025-08-10T15:03:44.959+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO BlockManagerMasterEndpoint: Registering block manager dab662e4aa8d:39737 with 434.4 MiB RAM, BlockManagerId(driver, dab662e4aa8d, 39737, None)
[2025-08-10T15:03:44.963+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dab662e4aa8d, 39737, None)
[2025-08-10T15:03:44.964+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dab662e4aa8d, 39737, None)
[2025-08-10T15:03:45.087+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250810150344-0000/0 on worker-20250810150056-172.18.0.4-35517 (172.18.0.4:35517) with 2 core(s)
[2025-08-10T15:03:45.090+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250810150344-0000/0 on hostPort 172.18.0.4:35517 with 2 core(s), 1024.0 MiB RAM
[2025-08-10T15:03:45.166+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-10T15:03:46.225+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250810150344-0000/0 is now RUNNING
[2025-08-10T15:03:50.386+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:50 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:39942) with ID 0, ResourceProfileId 0
[2025-08-10T15:03:50.511+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:50 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:32891 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 32891, None)
[2025-08-10T15:03:50.652+0000] {spark_submit.py:579} INFO - dict_keys(['daily', 'daily960', 'live_rapid', 'live_blitz', 'live_bullet', 'live_bughouse', 'live_blitz960', 'live_threecheck', 'live_crazyhouse', 'live_kingofthehill', 'tactics', 'rush', 'battle'])
[2025-08-10T15:03:50.664+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-10T15:03:50.667+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:50 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-08-10T15:03:52.522+0000] {spark_submit.py:579} INFO - Traceback (most recent call last):
[2025-08-10T15:03:52.522+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 84, in <module>
[2025-08-10T15:03:52.526+0000] {spark_submit.py:579} INFO - write_data(daily,"daily")
[2025-08-10T15:03:52.526+0000] {spark_submit.py:579} INFO - File "/opt/***/jobs/python/wordcountjob.py", line 80, in write_data
[2025-08-10T15:03:52.528+0000] {spark_submit.py:579} INFO - df.write.mode("overwrite").jdbc(url=jdbc_url, table=f"dbo.{table}", properties=properties)
[2025-08-10T15:03:52.528+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 2347, in jdbc
[2025-08-10T15:03:52.529+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2025-08-10T15:03:52.529+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
[2025-08-10T15:03:52.530+0000] {spark_submit.py:579} INFO - File "/home/***/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-08-10T15:03:52.573+0000] {spark_submit.py:579} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o138.jdbc.
[2025-08-10T15:03:52.574+0000] {spark_submit.py:579} INFO - : java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
[2025-08-10T15:03:52.574+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T15:03:52.575+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T15:03:52.575+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T15:03:52.575+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T15:03:52.576+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T15:03:52.576+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T15:03:52.576+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T15:03:52.577+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T15:03:52.577+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T15:03:52.577+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T15:03:52.578+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T15:03:52.578+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T15:03:52.578+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T15:03:52.579+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T15:03:52.579+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T15:03:52.579+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T15:03:52.580+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T15:03:52.580+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T15:03:52.580+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T15:03:52.580+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T15:03:52.581+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T15:03:52.581+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T15:03:52.581+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T15:03:52.581+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T15:03:52.582+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T15:03:52.582+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T15:03:52.582+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T15:03:52.583+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T15:03:52.583+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T15:03:52.583+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T15:03:52.583+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T15:03:52.583+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T15:03:52.584+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T15:03:52.584+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T15:03:52.584+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T15:03:52.584+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T15:03:52.585+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T15:03:52.585+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.585+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T15:03:52.586+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T15:03:52.586+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.586+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.586+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T15:03:52.587+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T15:03:52.587+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T15:03:52.587+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T15:03:52.587+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T15:03:52.587+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
[2025-08-10T15:03:52.588+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
[2025-08-10T15:03:52.588+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
[2025-08-10T15:03:52.588+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
[2025-08-10T15:03:52.588+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
[2025-08-10T15:03:52.589+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
[2025-08-10T15:03:52.589+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)
[2025-08-10T15:03:52.589+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)
[2025-08-10T15:03:52.589+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)
[2025-08-10T15:03:52.590+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-08-10T15:03:52.590+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-08-10T15:03:52.590+0000] {spark_submit.py:579} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-08-10T15:03:52.590+0000] {spark_submit.py:579} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-08-10T15:03:52.591+0000] {spark_submit.py:579} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-08-10T15:03:52.591+0000] {spark_submit.py:579} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-08-10T15:03:52.591+0000] {spark_submit.py:579} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-08-10T15:03:52.591+0000] {spark_submit.py:579} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-08-10T15:03:52.592+0000] {spark_submit.py:579} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-08-10T15:03:52.592+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-08-10T15:03:52.592+0000] {spark_submit.py:579} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-08-10T15:03:52.593+0000] {spark_submit.py:579} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-10T15:03:52.593+0000] {spark_submit.py:579} INFO - Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
[2025-08-10T15:03:52.593+0000] {spark_submit.py:579} INFO - at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
[2025-08-10T15:03:52.594+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
[2025-08-10T15:03:52.594+0000] {spark_submit.py:579} INFO - at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
[2025-08-10T15:03:52.594+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)
[2025-08-10T15:03:52.595+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)
[2025-08-10T15:03:52.596+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)
[2025-08-10T15:03:52.596+0000] {spark_submit.py:579} INFO - at scala.Option.foreach(Option.scala:437)
[2025-08-10T15:03:52.597+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)
[2025-08-10T15:03:52.597+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:272)
[2025-08-10T15:03:52.598+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:276)
[2025-08-10T15:03:52.598+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
[2025-08-10T15:03:52.599+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
[2025-08-10T15:03:52.599+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
[2025-08-10T15:03:52.600+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
[2025-08-10T15:03:52.600+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
[2025-08-10T15:03:52.601+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
[2025-08-10T15:03:52.602+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
[2025-08-10T15:03:52.603+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
[2025-08-10T15:03:52.603+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
[2025-08-10T15:03:52.603+0000] {spark_submit.py:579} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-10T15:03:52.604+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-08-10T15:03:52.604+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-08-10T15:03:52.604+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-08-10T15:03:52.605+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
[2025-08-10T15:03:52.605+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
[2025-08-10T15:03:52.606+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
[2025-08-10T15:03:52.606+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-08-10T15:03:52.606+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
[2025-08-10T15:03:52.607+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
[2025-08-10T15:03:52.608+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
[2025-08-10T15:03:52.608+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-08-10T15:03:52.609+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
[2025-08-10T15:03:52.610+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
[2025-08-10T15:03:52.610+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
[2025-08-10T15:03:52.611+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
[2025-08-10T15:03:52.613+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-08-10T15:03:52.617+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
[2025-08-10T15:03:52.618+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.619+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
[2025-08-10T15:03:52.619+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
[2025-08-10T15:03:52.620+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.620+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
[2025-08-10T15:03:52.621+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
[2025-08-10T15:03:52.621+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
[2025-08-10T15:03:52.621+0000] {spark_submit.py:579} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
[2025-08-10T15:03:52.622+0000] {spark_submit.py:579} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-08-10T15:03:52.623+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-08-10T15:03:52.624+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
[2025-08-10T15:03:52.624+0000] {spark_submit.py:579} INFO - at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
[2025-08-10T15:03:52.625+0000] {spark_submit.py:579} INFO - ... 20 more
[2025-08-10T15:03:52.625+0000] {spark_submit.py:579} INFO - 
[2025-08-10T15:03:52.772+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-10T15:03:52.774+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.
[2025-08-10T15:03:52.782+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO SparkUI: Stopped Spark web UI at http://dab662e4aa8d:4041
[2025-08-10T15:03:52.784+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-10T15:03:52.785+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-10T15:03:52.803+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-10T15:03:52.841+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-10T15:03:52.843+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO MemoryStore: MemoryStore cleared
[2025-08-10T15:03:52.844+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO BlockManager: BlockManager stopped
[2025-08-10T15:03:52.852+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-10T15:03:52.855+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-10T15:03:52.884+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO SparkContext: Successfully stopped SparkContext
[2025-08-10T15:03:52.884+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO ShutdownHookManager: Shutdown hook called
[2025-08-10T15:03:52.885+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO ShutdownHookManager: Deleting directory /opt/***/artifacts/spark-907a6403-6877-4e1d-9177-92c11a8201aa
[2025-08-10T15:03:52.893+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-3f6f0224-a292-4d0f-828c-9700da5c7430
[2025-08-10T15:03:52.907+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2b7e3d2-7b09-465f-b2a3-77c91d1d462f
[2025-08-10T15:03:52.914+0000] {spark_submit.py:579} INFO - 25/08/10 15:03:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-3f6f0224-a292-4d0f-828c-9700da5c7430/pyspark-8c23b670-ba06-4e4d-baa8-e0a122ef3139
[2025-08-10T15:03:52.966+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 505, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.
[2025-08-10T15:03:52.971+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=sparking_flow, task_id=python_job, execution_date=20250809T000000, start_date=20250810T150337, end_date=20250810T150352
[2025-08-10T15:03:52.983+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 5 for task python_job (Cannot execute: spark-submit --master spark://spark-master:7077 --jars /opt/spark/external-jars/mssql-jdbc-12.10.1.jre17.jar --name arrow-spark --deploy-mode client jobs/python/wordcountjob.py. Error code is: 1.; 213)
[2025-08-10T15:03:52.993+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-08-10T15:03:53.016+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
